{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKB26q-4-X8r"
      },
      "source": [
        "# Hugging Face Transformers Tutorial\n",
        "\n",
        "## Lab session for the course \"Introduction to Natural Language Processing (F21NL)\n",
        "\n",
        "This notebook will give an introduction to the Hugging Face Transformers Python library and some common patterns that you can use to take advantage of it. It is most useful for using or fine-tuning pretrained transformer models for your projects.\n",
        "\n",
        "\n",
        "[Hugging Face](https://huggingface.co/) provides access to:\n",
        "1. [models](https://huggingface.co/docs/transformers/index) containing both the code that implements them and their weights.\n",
        "2. [model-specific tokenizers](https://huggingface.co/docs/tokenizers/index).\n",
        "3. [pipelines](https://huggingface.co/docs/transformers/en/main_classes/pipelines) for common NLP tasks\n",
        "4. [datasets](https://huggingface.co/docs/datasets/en/index) as a separate `datasets` package to load existing datasets and built your own dataset.\n",
        "5. [metrics](https://huggingface.co/docs/evaluate/index) as a separate `evaluate` package to evaluate your moden using common NLP metrics.\n",
        "6. [training](https://huggingface.co/docs/accelerate/en/index) functionalities supporting GPU hardware as a separate `accelerate` package.\n",
        "\n",
        "all implemented mostly using PyTorch!\n",
        "\n",
        "We're going to go through a few use cases:\n",
        "* Overview of Tokenizers and Models\n",
        "* Finetuning - for your own task. We'll use a sentiment-classification example.\n",
        "\n",
        "\n",
        "Some of the benefits of using the Hugging Face ecosytem:\n",
        "\n",
        "1. Applying an existing pre-trained model to a new application or task and explore how to approach/solve it.\n",
        "2. Implementing a new or complex neural architecture and demonstrate its performance on some data.\n",
        "3. Analyzing the behavior of a model: how it represents linguistic knowledge or what kinds of phenomena it can handle or errors that it makes.\n",
        "\n",
        "Of these, `transformers` will be the most help for (1) and for (3). As we saw already in some previous labs, (2) involves a bit of learning curve but if you master it, you will find it very convenient to design a model based on existing ones provided by Huggingface. We won't be covering it here and please refer to [this example](https://huggingface.co/docs/transformers/en/custom_models).\n",
        "\n",
        "\n",
        "Additional Links:\n",
        "\n",
        "* [Hugging Face Docs](https://huggingface.co/docs/transformers/index)\n",
        "  * Clear documentation.\n",
        "  * Tutorials, walk-throughs, and example notebooks.\n",
        "  * List of available models.\n",
        "* [Hugging Face Course](https://huggingface.co/course/)\n",
        "    * Deep-dive into Large Language Models\n",
        "* [Hugging Face Examples](https://github.com/huggingface/transformers/tree/main/examples/pytorch)\n",
        "    * You can find very similar code structures accross very different downstream tasks/models using Huggingface.\n",
        "\n"
      ],
      "id": "sKB26q-4-X8r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9EhWoZef-X8u"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate bitsandbytes -U"
      ],
      "id": "9EhWoZef-X8u"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9th7mpc-X8v"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, Counter\n",
        "from typing import Any\n",
        "import json\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def print_encoding(model_inputs, indent=4):\n",
        "    indent_str = \" \" * indent\n",
        "    print(\"{\")\n",
        "    for k, v in model_inputs.items():\n",
        "        print(indent_str + k + \":\")\n",
        "        print(indent_str + indent_str + str(v))\n",
        "    print(\"}\")"
      ],
      "id": "Q9th7mpc-X8v"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmXezUMg2idv"
      },
      "source": [
        "## Part 1: Common Pattern for using Hugging Face Transformers\n",
        "\n",
        "We're going to start off with a common usage pattern for Hugging Face Transformers, using the example of Sentiment Analysis.\n",
        "\n",
        "Given a sentence, the goal is to predict the sentiment of that sentence (either positive or negative):\n",
        "\n",
        "* This movie is awesome, I can watch it all the time, without getting bored -> Positive\n",
        "* This movie is horrible, I cannot believe I wasted my time watching it -> Negative\n",
        "\n",
        "First, find a model on [the hub](https://huggingface.co/models). For the purpose of this tutorial, Anyone can we are going to use a model from [this paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3489963)).\n",
        "\n",
        "Then, there are two objects that need to be initialized - a **tokenizer**, and a **model**\n",
        "\n",
        "* Tokenizer converts strings to lists of vocabulary ids that the model requires\n",
        "* Model takes the vocabulary ids and produces a prediction"
      ],
      "id": "qmXezUMg2idv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySLmJ0Z-oD35"
      },
      "source": [
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![full_nlp_pipeline.png](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)\n",
        "\n",
        "\n",
        "From [https://huggingface.co/course/chapter2/2?fw=pt](https://huggingface.co/course/chapter2/2?fw=pt)"
      ],
      "id": "ySLmJ0Z-oD35"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mcsii_O42Z8Q"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"siebert/sentiment-roberta-large-english\"\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Initialize the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "print(model)\n",
        "\n",
        "# The models from Hugging Face are torch.nn.Modules!\n",
        "print(isinstance(model, torch.nn.Module))"
      ],
      "id": "Mcsii_O42Z8Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the above cell, this model `RobertaForSequenceClassification` is composed of three submodules: `RobertaModel`, `RobertaEncoder` and a `RobertaClassificationHead`:\n",
        "\n",
        "The model accepts as input sequences containing input ids from a vocabulary of 50265 tokens (see the `word_embeddings` and classifies the sequences into 2 classes (see the final layer of the classifier).\n",
        "\n",
        "In particular:\n",
        "\n",
        "1. The `RobertaModel` embeds these sequences with `word_embeddings` and adds positional information to every token in the sequence.\n",
        "2. The `RobertaEncoder` contains several layers that transform the embedded sequences.\n",
        "3. The `RobertaClassificationHead` is an MLP accepts the transformed output from the final layer of the `RobertaEncoder` and produces a dimensional logit vector for each sequence.\n",
        "\n",
        "\n",
        "Do not worry if you do not fully understand the flow of the input or some individual components of the model for now. We will talk about them in the next lab where we analyze the anatomy of a transformer."
      ],
      "metadata": {
        "id": "dsXvDZNKKGID"
      },
      "id": "dsXvDZNKKGID"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT_zeWRBoD36"
      },
      "outputs": [],
      "source": [
        "# Lets tokenize an example sentence and perform a forward pass on the model\n",
        "\n",
        "# @markdown Type a sentence here and get the sentiment of that sentence from the model!\n",
        "inputs = \"This movie sucks\" # @param {\"type\":\"string\"}\n",
        "tokenized_inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
        "\n",
        "# Perform a forward pass on the model\n",
        "outputs = model(**tokenized_inputs)\n",
        "\n",
        "labels = ['NEGATIVE', 'POSITIVE']\n",
        "prediction = torch.argmax(outputs.logits)\n",
        "\n",
        "print(\"Input:\")\n",
        "print(inputs)\n",
        "print()\n",
        "print(\"Tokenized Inputs: {tokenized_inputs}\")\n",
        "print_encoding(tokenized_inputs)\n",
        "print()\n",
        "print(\"Model Outputs:\")\n",
        "print(outputs)\n",
        "print()\n",
        "print(f\"The prediction is {labels[prediction]}\")"
      ],
      "id": "kT_zeWRBoD36"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7jvH9haoD37"
      },
      "source": [
        "## Tokenizers"
      ],
      "id": "a7jvH9haoD37"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43FLbwgz-X83"
      },
      "source": [
        "Pretrained models are implemented along with **tokenizers** that are used to preprocess their inputs. The tokenizers take raw strings or list of strings and output what are effectively dictionaries that contain the the model inputs.\n",
        "\n",
        "\n",
        "You can access tokenizers either with the Tokenizer class specific to the model you want to use (here DistilBERT), or with the AutoTokenizer class.\n",
        "Fast Tokenizers are written in Rust, while their slow versions are written in Python."
      ],
      "id": "43FLbwgz-X83"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pu6L0lWG-X83",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertTokenizerFast, AutoTokenizer\n",
        "name = \"distilbert/distilbert-base-cased\"\n",
        "# name = `user/name when loading from the Hugging Face hub\n",
        "# name = `local_path` when loading from local\n",
        "\n",
        "# Tokenizer written in Python (slow)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(name)\n",
        "print(tokenizer)\n",
        "# Tokenizer written in Rust (fast)\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(name)\n",
        "print(tokenizer)\n",
        "# Convenient! Defaults to Fast\n",
        "tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "print(tokenizer)"
      ],
      "id": "Pu6L0lWG-X83"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that this particular tokenizer has sever attributes:\n",
        "\n",
        "1. Has a `vocab_size` of 28996 tokens\n",
        "2. The `model_max_length` is 1000000000000000019884624838656, this is the maximum length of a sequence before the tokenizer truncates it. Any sequence with more than `model_max_length` tokens will be truncated to that value.\n",
        "3. `padding_side` is set to right. We have not talked about padding yet, but when preparing a batch of sentences, some of them will have different lengths. Padding ensures that all sequences within the batch will have the same length and therefore can be processed by the model at the same time.\n",
        "4. `truncation_side` is set to right when truncating long sequences\n",
        "5. The model has a lot of special tokens `[PAD], [UNK], [CLS], [SEP], [MASK]`, just like the `<START>` and `<END>` tokens that we used in our bi-gram language model."
      ],
      "metadata": {
        "id": "3wFydXk0Otzb"
      },
      "id": "3wFydXk0Otzb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrPzbBhR-X84"
      },
      "outputs": [],
      "source": [
        "# This is how you call the tokenizer\n",
        "input_str = \"Hugging Face Transformers is great!\"\n",
        "tokenized_inputs = tokenizer(input_str)\n",
        "\n",
        "\n",
        "print(\"Vanilla Tokenization\")\n",
        "print_encoding(tokenized_inputs)\n",
        "print()\n",
        "\n",
        "# Accessing outputs of the tokenizer:\n",
        "print(tokenized_inputs.input_ids)\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ],
      "id": "zrPzbBhR-X84"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How tokenization works under the hood?\n",
        "\n",
        "Tokenization happens in a few steps:"
      ],
      "metadata": {
        "id": "izq0_DlLSE8P"
      },
      "id": "izq0_DlLSE8P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_8C6L2G-X85"
      },
      "outputs": [],
      "source": [
        "# Step 1: Converting input sentences into token ids\n",
        "input_tokens = tokenizer.tokenize(input_str)\n",
        "input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
        "# Step 2: Appending start and end of sequence tokens\n",
        "input_ids_special_tokens = [tokenizer.cls_token_id] + input_ids + [tokenizer.sep_token_id]\n",
        "\n",
        "# Converting back token ids into sentences\n",
        "decoded_str = tokenizer.decode(input_ids_special_tokens)\n",
        "\n",
        "print(\"start:                \", input_str)\n",
        "print(\"tokenize:             \", input_tokens)\n",
        "print(\"convert_tokens_to_ids:\", input_ids)\n",
        "print(\"add special tokens:   \", input_ids_special_tokens)\n",
        "print(\"--------\")\n",
        "print(\"decode:               \", decoded_str)"
      ],
      "id": "E_8C6L2G-X85"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer can return pytorch tensors"
      ],
      "metadata": {
        "id": "uMAzXC1sTBdj"
      },
      "id": "uMAzXC1sTBdj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vt5WV-6S-X87"
      },
      "outputs": [],
      "source": [
        "model_inputs = tokenizer(\"Hugging Face Transformers is great!\", return_tensors=\"pt\")\n",
        "print(\"PyTorch Tensors:\")\n",
        "print_encoding(model_inputs)"
      ],
      "id": "vt5WV-6S-X87"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passing Multiple inputs at the same time"
      ],
      "metadata": {
        "id": "kpNdg5tFTGAP"
      },
      "id": "kpNdg5tFTGAP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI3bAzpeoD3_"
      },
      "outputs": [],
      "source": [
        "# You can pass multiple strings into the tokenizer and pad them as you need\n",
        "model_inputs = tokenizer(\n",
        "    [\n",
        "        \"Hugging Face Transformers is great!\",\n",
        "        \"The quick brown fox jumps over the lazy dog. Then the dog got up and ran away because she didn't like foxes.\",\n",
        "    ],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")\n",
        "print(f\"Pad token: {tokenizer.pad_token} | Pad token id: {tokenizer.pad_token_id}\")\n",
        "print(\"Padding:\")\n",
        "\n",
        "# Notice how the tokenizer automatically converts the inputs of variable length into sequences by padding!\n",
        "print(model_inputs.input_ids.shape)\n",
        "print_encoding(model_inputs)"
      ],
      "id": "HI3bAzpeoD3_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "iSZat-nkoD3_"
      },
      "outputs": [],
      "source": [
        "# You can also decode a whole batch at once:\n",
        "print(\"Batch Decode:\")\n",
        "print(tokenizer.batch_decode(model_inputs.input_ids))\n",
        "print()\n",
        "print(\"Batch Decode: (no special characters)\")\n",
        "print(tokenizer.batch_decode(model_inputs.input_ids, skip_special_tokens=True))"
      ],
      "id": "iSZat-nkoD3_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmZNLz_noD4A"
      },
      "source": [
        "For more information about tokenizers, you can look at:\n",
        "[Hugging Face Transformers Docs](https://huggingface.co/docs/transformers/main_classes/tokenizer) and the [Hugging Face Tokenizers Library](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html) (For the Fast Tokenizers). The Tokenizers Library even lets you train your own tokenizers!"
      ],
      "id": "JmZNLz_noD4A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The role of attention mask\n",
        "\n",
        "Perhaps you have noticed that along with the `input_ids` the tokenizer also returns an `attention_mask`. What is this `attention_mask`?\n",
        "\n",
        "The vast majority of the models that are being developed nowadays are based on the Transformer architecture. At its core, the Transformer, relies on the `self attention` operation (you will learn about it more in detail in this week's lecture and the follow-up lab), where the individual tokens within the sequence will look at other tokens in the sequence to find useful information.\n",
        "\n",
        "However, the because we apply padding, we do not want the non-padded tokens in the sequence to look at the padded ones, because the padding token is a **hack to enable batch processing via multiplying the inputs with the weights a linear layer**.\n",
        "\n",
        "Suppose we have the two following examples in a batch that are tokenized with whitespace:\n",
        "\n",
        "1. The plot was bad, I was bored in the end. (10 tokens)\n",
        "\n",
        "    [`The` `plot` `was` `bad` `I` `was` `bored` `in` `the` `end`]\n",
        "\n",
        "2. This movie was nice, it made me feel happy. (9 tokens + adding 1 one padding for batch processing)\n",
        "\n",
        "    [`This` `movie` `was` `nice` `it` `made` `me` `feel` `happy` `<pad>`]\n",
        "\n",
        "\n",
        "For the second sentence, we would most likely expect that the the word `happy` should look into other words like `nice`, or `feel` rather than the `padding` token.\n",
        "\n",
        "\n",
        "We will come back to the role of the attention and its implementation later on. For now this section was done for explaining the `attention_mask` output of the tokenizer."
      ],
      "metadata": {
        "id": "OQj6aw23UDJV"
      },
      "id": "OQj6aw23UDJV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6juLjnNt-X87"
      },
      "source": [
        "## Models\n",
        "\n",
        "\n",
        "Initializing models is very similar to initializing tokenizers. You can either use the model class specific to your model or you can use an [AutoModel](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html) class.\n",
        "\n",
        "\n",
        "While most of the pretrained transformers have similar architecture, if you there are additional weights, called \"heads\" that you have to train if you're doing sequence classification, question answering, or some other task. Hugging Face automatically sets up the architecture you need when you specify the model class.\n",
        "\n",
        "For example, we are doing sentiment analysis, so we are going to use `DistilBertForSequenceClassification`. If we were going to continue training DistilBERT on its masked-language modeling training objective, we would use `DistilBertForMaskedLM`, and if we just wanted the model's representations, maybe for our own downstream task, we could just use `DistilBertModel`.\n",
        "\n",
        "\n",
        "Here's a stylized picture of a model recreated from one found here: [https://huggingface.co/course/chapter2/2?fw=pt](https://huggingface.co/course/chapter2/2?fw=pt).\n",
        "![model_illustration.png](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg)\n",
        "\n",
        "\n",
        "\n",
        "There are three types of models:\n",
        "* Encoder-only (e.g. BERT)\n",
        "* Decoder-only (e.g. GPT2)\n",
        "* Encoder-Decoder models (e.g. BART or T5)\n",
        "\n",
        "The task-specific classes you have available depend on what type of model you're dealing with.\n",
        "\n",
        "\n",
        "A full list of choices are available in the [docs](https://huggingface.co/docs/transformers/model_doc/auto).\n"
      ],
      "id": "6juLjnNt-X87"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXm1K2sF-X88",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, DistilBertForSequenceClassification, DistilBertModel\n",
        "print(\"Loading base model\")\n",
        "base_model = DistilBertModel.from_pretrained(\"distilbert-base-cased\")\n",
        "\n",
        "print(\"Loading classification model from base model;s checkpoint\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels=2)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels=2)\n"
      ],
      "id": "RXm1K2sF-X88"
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also initialize with random weights"
      ],
      "metadata": {
        "id": "v_5IMvUvgEWU"
      },
      "id": "v_5IMvUvgEWU"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertConfig, DistilBertModel\n",
        "\n",
        "# Initializing a DistilBERT configuration\n",
        "configuration = DistilBertConfig()\n",
        "configuration.num_labels=2\n",
        "# Initializing a model (with random weights) from the configuration\n",
        "model = DistilBertForSequenceClassification(configuration)\n",
        "\n",
        "# Accessing the model configuration\n",
        "configuration = model.config"
      ],
      "metadata": {
        "id": "IkqSkIhKgIrZ"
      },
      "id": "IkqSkIhKgIrZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1opFV7Vi-X88"
      },
      "source": [
        "We get a warning here because the sequence classification parameters haven't been trained yet.\n",
        "\n",
        "Passing inputs to the model is super easy. They take inputs as keyword arguments"
      ],
      "id": "1opFV7Vi-X88"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDZ72k-U-X89"
      },
      "outputs": [],
      "source": [
        "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
        "\n",
        "# Option 1: using the keys directly as arguments\n",
        "model_outputs = model(input_ids=model_inputs.input_ids, attention_mask=model_inputs.attention_mask)\n",
        "\n",
        "# Option 2: the keys of the dictionary the tokenizer returns are the same as the keyword arguments the model expects\n",
        "# f({k1: v1, k2: v2}) = f(k1=v1, k2=v2)\n",
        "# And so we a can use the ** operation which assigns the values directly to the keyword arguments of the model\n",
        "model_outputs = model(**model_inputs)\n",
        "\n",
        "print(model_inputs)\n",
        "print()\n",
        "print(model_outputs)\n",
        "print()\n",
        "print(f\"Distribution over labels: {torch.softmax(model_outputs.logits, dim=1)}\")"
      ],
      "id": "TDZ72k-U-X89"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRRqhVUloD4C"
      },
      "source": [
        "If you notice, it's a bit weird that we have two classes for a binary classification task - you could easily have a single class and just choose a threshold. It's like this because of how huggingface models calculate the loss. This will increase the number of parameters we have, but shouldn't otherwise affect performance."
      ],
      "id": "oRRqhVUloD4C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvie4gYD-X8-"
      },
      "source": [
        "These models are just Pytorch Modules!\n",
        "\n",
        "* You can can calculate the loss with your `loss_func` and call `loss.backward`.\n",
        "* You can use any of the optimizers or learning rate schedulers that you used"
      ],
      "id": "rvie4gYD-X8-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Irxo7sDboD4C"
      },
      "outputs": [],
      "source": [
        "# You can calculate the loss like normal\n",
        "label = torch.tensor([1])\n",
        "loss = torch.nn.functional.cross_entropy(model_outputs.logits, label)\n",
        "print(loss)\n",
        "loss.backward()\n",
        "\n",
        "# You can get the parameters\n",
        "list(model.named_parameters())[0]"
      ],
      "id": "Irxo7sDboD4C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpHeG1zDoD4D"
      },
      "source": [
        "Hugging Face provides an additional easy way to calculate the loss as well:"
      ],
      "id": "mpHeG1zDoD4D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S148gCyG-X8-"
      },
      "outputs": [],
      "source": [
        "# To calculate the loss, we need to pass in a label:\n",
        "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
        "\n",
        "labels = ['NEGATIVE', 'POSITIVE']\n",
        "model_inputs['labels'] = torch.tensor([1])\n",
        "\n",
        "model_outputs = model(**model_inputs)\n",
        "\n",
        "\n",
        "print(model_outputs)\n",
        "print()\n",
        "print(f\"Model predictions: {labels[model_outputs.logits.argmax()]}\")"
      ],
      "id": "S148gCyG-X8-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y6E3IxzoD4E"
      },
      "source": [
        "One final note - you can get the hidden states and attention weights from the models really easily. This is particularly helpful if you're working on an analysis project. (For example, see [What does BERT look at?](https://arxiv.org/abs/1906.04341))."
      ],
      "id": "7Y6E3IxzoD4E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WzqhpquoD4E"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(\"distilbert-base-cased\", output_attentions=True, output_hidden_states=True)\n",
        "model.eval()\n",
        "\n",
        "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    model_output = model(**model_inputs)\n",
        "\n",
        "\n",
        "print(\"Hidden state size (per layer):  \", model_output.hidden_states[0].shape)\n",
        "# (layer, batch, query_word_idx, key_word_idxs), y-axis is query, x-axis is key\n",
        "print(\"Attention head size (per layer):\", model_output.attentions[0].shape)\n",
        "\n",
        "# print(model_output)"
      ],
      "id": "5WzqhpquoD4E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH_MAK-soD4F"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(model_inputs.input_ids[0])\n",
        "print(tokens)\n",
        "\n",
        "\n",
        "n_layers = len(model_output.attentions)\n",
        "n_heads = len(model_output.attentions[0][0])\n",
        "fig, axes = plt.subplots(6, 12)\n",
        "fig.set_size_inches(18.5*2, 10.5*2)\n",
        "for layer in range(n_layers):\n",
        "    for i in range(n_heads):\n",
        "        axes[layer, i].imshow(model_output.attentions[layer][0, i])\n",
        "        axes[layer][i].set_xticks(list(range(9)))\n",
        "        axes[layer][i].set_xticklabels(labels=tokens, rotation=\"vertical\")\n",
        "        axes[layer][i].set_yticks(list(range(9)))\n",
        "        axes[layer][i].set_yticklabels(labels=tokens)\n",
        "\n",
        "        if layer == 5:\n",
        "            axes[layer, i].set(xlabel=f\"head={i}\")\n",
        "        if i == 0:\n",
        "            axes[layer, i].set(ylabel=f\"layer={layer}\")\n",
        "\n",
        "plt.subplots_adjust(wspace=0.3)\n",
        "plt.show()"
      ],
      "id": "SH_MAK-soD4F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uumcErs2-X80"
      },
      "source": [
        "## Part 2: Finetuning\n",
        "\n",
        "For your projects, you are much more likely to want to finetune a pretrained model. This is a little bit more involved, but is still quite easy."
      ],
      "id": "uumcErs2-X80"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDdGp4Ua-X81"
      },
      "source": [
        "### 2.1 Loading in a dataset\n",
        "\n",
        "In addition to having models, the [the hub](https://huggingface.co/datasets) also has datasets."
      ],
      "id": "WDdGp4Ua-X81"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTsW-Wwi-X81"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# DataLoader(zip(list1, list2))\n",
        "dataset_name = \"stanfordnlp/imdb\"\n",
        "\n",
        "imdb_dataset = load_dataset(dataset_name)\n",
        "\n",
        "\n",
        "# Just take the first 50 tokens for speed/running on cpu\n",
        "def truncate(example: dict[str, Any]) -> dict[str, str]:\n",
        "    return {\n",
        "        'text': \" \".join(example['text'].split()[:50]),\n",
        "        'label': example['label']\n",
        "    }\n",
        "\n",
        "imdb_dataset"
      ],
      "id": "OTsW-Wwi-X81"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCaX-gNo0OEV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Take 128 random examples for train and 32 validation\n",
        "small_imdb_dataset = DatasetDict(\n",
        "    train=imdb_dataset['train'].shuffle(seed=1111).select(range(128)).map(truncate),\n",
        "    val=imdb_dataset['train'].shuffle(seed=1111).select(range(128, 160)).map(truncate),\n",
        ")\n",
        "small_imdb_dataset"
      ],
      "id": "vCaX-gNo0OEV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBS4c44A-X82"
      },
      "outputs": [],
      "source": [
        "small_imdb_dataset['train'][:10]"
      ],
      "id": "bBS4c44A-X82"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bjqop3N-X8_"
      },
      "outputs": [],
      "source": [
        "# Prepare the dataset - this tokenizes the dataset in batches of 16 examples.\n",
        "small_tokenized_dataset = small_imdb_dataset.map(\n",
        "    lambda example: tokenizer(example['text'], padding=True, truncation=True), # https://huggingface.co/docs/transformers/pad_truncation\n",
        "    batched=True,\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "small_tokenized_dataset = small_tokenized_dataset.remove_columns([\"text\"])\n",
        "small_tokenized_dataset = small_tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "small_tokenized_dataset.set_format(\"torch\")"
      ],
      "id": "3bjqop3N-X8_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "450eUYlf-X8_"
      },
      "outputs": [],
      "source": [
        "small_tokenized_dataset['train'][0:2]"
      ],
      "id": "450eUYlf-X8_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3e7_htt-X8_"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(small_tokenized_dataset['train'], batch_size=16)\n",
        "eval_dataloader = DataLoader(small_tokenized_dataset['val'], batch_size=16)"
      ],
      "id": "Q3e7_htt-X8_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRm6Tw_z-X8-"
      },
      "source": [
        "### 2.2 Training\n",
        "\n",
        "To train your models, you can just use the same kind of training loop that you would use in Pytorch.\n",
        "\n",
        "Hugging Face models are also `torch.nn.Module`s so backpropagation happens the same way and you can even use the same optimizers. Hugging Face also includes learning rate schedules that were used to train Transformer models, so you can use these too.\n",
        "\n",
        "For optimization, we're using the AdamW Optimizer, and a linear learning rate scheduler, which reduces the learning rate a little bit after each training step over the course of training."
      ],
      "id": "qRm6Tw_z-X8-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7CvjTIv-X9A"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\n",
        "\n",
        "num_epochs = 1\n",
        "num_training_steps = len(train_dataloader)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
        "print(optimizer)\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "for epoch in range(num_epochs):\n",
        "    # training\n",
        "    model.train()\n",
        "    for batch_i, batch in enumerate(train_dataloader):\n",
        "        # batch = ([text1, text2], [0, 1])\n",
        "\n",
        "        # Step 1: forward pass\n",
        "        output = model(**batch)\n",
        "\n",
        "        # Step 2: Zero gradients for all parameters\n",
        "        optimizer.zero_grad()\n",
        "        # Step 3: Compute loss\n",
        "        output.loss.backward()\n",
        "        # Step 4: Update weights\n",
        "        optimizer.step()\n",
        "        # Step 5: Adjust the learning rate\n",
        "        lr_scheduler.step()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    for batch_i, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            output = model(**batch)\n",
        "        loss += output.loss\n",
        "\n",
        "    avg_val_loss = loss / len(eval_dataloader)\n",
        "    print(f\"Validation loss: {avg_val_loss}\")\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        print(\"Saving checkpoint!\")\n",
        "        best_val_loss = avg_val_loss\n",
        "        # torch.save({\n",
        "        #     'epoch': epoch,\n",
        "        #     'model_state_dict': model.state_dict(),\n",
        "        #     'optimizer_state_dict': optimizer.state_dict(),\n",
        "        #     'val_loss': best_val_loss,\n",
        "        #     },\n",
        "        #     f\"checkpoints/epoch_{epoch}.pt\"\n",
        "        # )"
      ],
      "id": "A7CvjTIv-X9A"
    },
    {
      "cell_type": "code",
      "source": [
        "batch['input_ids'].max()"
      ],
      "metadata": {
        "id": "cl_JX8HKbVmb"
      },
      "id": "cl_JX8HKbVmb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrfE9c04-X9A"
      },
      "source": [
        "While you can use PyTorch to train your models like we, Hugging Face offers a powerful [`Trainer`](https://huggingface.co/docs/transformers/en/main_classes/trainer) class to handle most needs."
      ],
      "id": "YrfE9c04-X9A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQDkqtT7-X9B"
      },
      "outputs": [],
      "source": [
        "imdb_dataset = load_dataset(\"stanfordnlp/imdb\")\n",
        "\n",
        "small_imdb_dataset = DatasetDict(\n",
        "    train=imdb_dataset['train'].shuffle(seed=1111).select(range(128)).map(truncate),\n",
        "    val=imdb_dataset['train'].shuffle(seed=1111).select(range(128, 160)).map(truncate),\n",
        ")\n",
        "\n",
        "small_tokenized_dataset = small_imdb_dataset.map(\n",
        "    lambda example: tokenizer(example['text'], truncation=True),\n",
        "    batched=True,\n",
        "    batch_size=16\n",
        ")"
      ],
      "id": "fQDkqtT7-X9B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RULukU5eoD4M"
      },
      "source": [
        "`TrainingArguments` specifies different training parameters like how often to evaluate and save model checkpoints, where to save them, etc. There are **many** aspects you can customize and it's worth checking them out [here](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments).\n",
        "\n",
        "Some things you can control include:\n",
        "* learning rate, weight decay, gradient clipping,\n",
        "* checkpointing, logging, and evaluation frequency\n",
        "\n",
        "The `Trainer` actually performs the training. You can pass it the `TrainingArguments`, model, the datasets, tokenizer, optimizer, and even model checkpoints to resume training from. The `compute_metrics` function is called at the end of evaluation/validation to calculate evaluation metrics."
      ],
      "id": "RULukU5eoD4M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEmERCu_-X9B",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Dont use wandb to log the experiment\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\n",
        "\n",
        "arguments = TrainingArguments(\n",
        "    output_dir=\"sample_hf_trainer\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    eval_strategy=\"epoch\", # run validation at the end of each epoch\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    load_best_model_at_end=True,\n",
        "    seed=224,\n",
        "    log_level=\"debug\",\n",
        ")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # calculates the accuracy\n",
        "    return {\"accuracy\": np.mean(predictions == labels)}\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=arguments,\n",
        "    train_dataset=small_tokenized_dataset['train'],\n",
        "    eval_dataset=small_tokenized_dataset['val'], # change to test when you do your final evaluation!\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "id": "FEmERCu_-X9B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbyK1W95-X9B"
      },
      "source": [
        "#### Callbacks: Logging and Early Stopping\n",
        "\n",
        "\n",
        "Hugging Face Transformers also allows you to write `Callbacks` if you want certain things to happen at different points during training (e.g. after evaluation or after an epoch has finished). For example, there is a callback for early stopping, and I usually write one for logging as well.\n",
        "\n",
        "For more information on callbacks see [here](https://huggingface.co/docs/transformers/main_classes/callback#transformers.TrainerCallback)."
      ],
      "id": "MbyK1W95-X9B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKO3TkAnoD4N"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainerCallback, EarlyStoppingCallback\n",
        "\n",
        "class LoggingCallback(TrainerCallback):\n",
        "    def __init__(self, log_path):\n",
        "        self.log_path = log_path\n",
        "\n",
        "    # will call on_log on each logging step, specified by TrainerArgument. (i.e TrainerArguement.logginng_step)\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        _ = logs.pop(\"total_flos\", None)\n",
        "        if state.is_local_process_zero:\n",
        "            with open(self.log_path, \"a\") as f:\n",
        "                f.write(json.dumps(logs) + \"\\n\")\n",
        "    # def on_epoch(...)\n",
        "\n",
        "\n",
        "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=1, early_stopping_threshold=0.0))\n",
        "trainer.add_callback(LoggingCallback(\"sample_hf_trainer/log.jsonl\"))"
      ],
      "id": "vKO3TkAnoD4N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tunwonc2-X9C",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "trainer.train()"
      ],
      "id": "tunwonc2-X9C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q185j9V-X9C"
      },
      "outputs": [],
      "source": [
        "# evaluating the model is very easy\n",
        "# just gets evaluation metrics\n",
        "results = trainer.evaluate()\n",
        "results = trainer.predict(small_tokenized_dataset['val']) # also gives you predictions"
      ],
      "id": "_Q185j9V-X9C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ0aGxeh-X9D"
      },
      "outputs": [],
      "source": [
        "results"
      ],
      "id": "UJ0aGxeh-X9D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSGsZo3xoD4O"
      },
      "outputs": [],
      "source": [
        "# To load our saved model, we can pass the path to the checkpoint into the `from_pretrained` method:\n",
        "test_str = \"I enjoyed the movie!\"\n",
        "\n",
        "finetuned_model = AutoModelForSequenceClassification.from_pretrained(\"sample_hf_trainer/checkpoint-8\")\n",
        "model_inputs = tokenizer(test_str, return_tensors=\"pt\")\n",
        "prediction = torch.argmax(finetuned_model(**model_inputs).logits)\n",
        "print([\"NEGATIVE\", \"POSITIVE\"][prediction])"
      ],
      "id": "kSGsZo3xoD4O"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MllSTgehoD4O"
      },
      "source": [
        "Included here are also some practical tips for fine-tuning:\n",
        "\n",
        "**Good default hyperparameters.** The hyperparameters you will depend on your task and dataset. You should do a hyperparameter search to find the best ones. That said, here are some good initial values for fine-tuning.\n",
        "* Epochs: {2, 3, 4} (larger amounts of data need fewer epochs)\n",
        "* Batch size (bigger is better: as large as you can make it)\n",
        "* Optimizer: AdamW\n",
        "* AdamW learning rate: {2e-5, 5e-5}\n",
        "* Learning rate scheduler: linear warm up for first {3-10%} steps of training\n",
        "* weight_decay (l2 regularization): {0, 0.01, 0.1}\n",
        "\n",
        "You should monitor your validation loss to decide when you've found good hyperparameters."
      ],
      "id": "MllSTgehoD4O"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nCrUosgoD4P"
      },
      "source": [
        "## Part 3: Generation\n",
        "\n",
        "In the example above we finetuned the model on a classification task, but you can also finetune models on language modeling tasks, where we predict the probability distribution of the next token in a sequence.\n",
        "\n",
        "The [`generate`](https://huggingface.co/docs/transformers/v4.56.1/en/main_classes/text_generation#transformers.GenerationMixin.generate) function makes it easy to generate from these models."
      ],
      "id": "9nCrUosgoD4P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfQEV8EKoD4P"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "gpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "\n",
        "gpt2 = AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
        "gpt2.config.pad_token_id = gpt2.config.eos_token_id  # Prevents warning during decoding"
      ],
      "id": "QfQEV8EKoD4P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5wo61xmoD4Q"
      },
      "outputs": [],
      "source": [
        "prompt = \"Once upon a time\"\n",
        "\n",
        "tokenized_prompt = gpt2_tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "for i in range(10):\n",
        "    output = gpt2.generate(\n",
        "        **tokenized_prompt,\n",
        "        max_length=50,\n",
        "        do_sample=True,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    print(f\"{i + 1}) {gpt2_tokenizer.batch_decode(output)[0]}\")"
      ],
      "id": "G5wo61xmoD4Q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLAHLU4q9HYQ"
      },
      "source": [
        "## Part 4: Defining Custom Datasets\n",
        "\n",
        "There are a few ways to go about defining datasets, but I'm going to show an example using Pytorch Dataloaders. This example uses an encoder-decoder dataset,the [E2E Dataset](https://arxiv.org/abs/1706.09254), which is maps structured information about restaurants to natural language descriptions."
      ],
      "id": "QLAHLU4q9HYQ"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/tuetschek/e2e-dataset/refs/heads/master/trainset.csv"
      ],
      "metadata": {
        "id": "MM5b7zQhwSEv"
      },
      "id": "MM5b7zQhwSEv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLqz11UioD4Q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "df = pd.read_csv(\"trainset.csv\")\n",
        "custom_dataset = Dataset.from_pandas(df)"
      ],
      "id": "MLqz11UioD4Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRauc5JBoD4R"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class E2EDataset(Dataset):\n",
        "    \"\"\"Tokenize data when we call __getitem__\"\"\"\n",
        "    def __init__(self, path, tokenizer):\n",
        "        with open(path, newline=\"\") as f:\n",
        "            reader = csv.reader(f)\n",
        "            next(reader) # skip the heading\n",
        "            self.data = [{\"source\": row[0], \"target\": row[1]} for row in reader]\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        inputs = self.tokenizer(self.data[i]['source'])\n",
        "        labels = self.tokenizer(self.data[i]['target'])\n",
        "        inputs['labels'] = labels.input_ids\n",
        "        return inputs\n"
      ],
      "id": "lRauc5JBoD4R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eRu5mFIpoD4R"
      },
      "outputs": [],
      "source": [
        "bart_tokenizer = AutoTokenizer.from_pretrained('facebook/bart-base')"
      ],
      "id": "eRu5mFIpoD4R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I32zL1nEoD4S"
      },
      "outputs": [],
      "source": [
        "dataset = E2EDataset(\"trainset.csv\", bart_tokenizer)"
      ],
      "id": "I32zL1nEoD4S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8frTRD3oD4T"
      },
      "outputs": [],
      "source": [
        "dataset[0]"
      ],
      "id": "w8frTRD3oD4T"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHI3KuNZ-X8w"
      },
      "source": [
        "## Part 5: Pipelines\n",
        "\n",
        "There are some standard NLP tasks like sentiment classification or question answering where there are already pre-trained (and fine-tuned!) models available through Hugging Face Transformer's [_Pipeline_](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/pipelines#transformers.pipeline) interface.\n",
        "\n",
        "For your projects, you likely won't be using it too much, but it's still worth knowing about!\n",
        "\n",
        "Here's an example with Sentiment Analysis:"
      ],
      "id": "tHI3KuNZ-X8w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gOj5ODS0-X8x"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"siebert/sentiment-roberta-large-english\")"
      ],
      "id": "gOj5ODS0-X8x"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5wZuMG2-X8y"
      },
      "source": [
        "You can run the pipeline by just calling it on a string"
      ],
      "id": "D5wZuMG2-X8y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrygLkiQ-X8y"
      },
      "outputs": [],
      "source": [
        "sentiment_analysis(\"Hugging Face Transformers is really cool!\")"
      ],
      "id": "GrygLkiQ-X8y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e2E8qKH-X8z"
      },
      "source": [
        "Or on a list of strings:"
      ],
      "id": "0e2E8qKH-X8z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpBvCpVM-X8z"
      },
      "outputs": [],
      "source": [
        "sentiment_analysis(\n",
        "    [\n",
        "        \"I didn't know if I would like Hkarl, but it turned out pretty good.\",\n",
        "        \"I didn't know if I would like Hkarl, and it was just as bad as I'd heard.\"\n",
        "    ]\n",
        ")"
      ],
      "id": "EpBvCpVM-X8z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ptc0BViy-X80"
      },
      "source": [
        "You can find more information on pipelines (including which ones are available) [here](https://huggingface.co/docs/transformers/main_classes/pipelines)"
      ],
      "id": "Ptc0BViy-X80"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}